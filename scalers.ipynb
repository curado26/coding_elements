{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "Feature scaling is a crucial step in the preprocessing pipeline for many \n",
    "statistical modeling and machine learning algorithms. Scalers transform features \n",
    "so that they are on the same scale, making it easier for the models to converge \n",
    "and perform optimally.\n",
    "\n",
    "Importance:\n",
    "\n",
    "* **Consistency** across features: without scaling , algorithms may become biased toward the larger-scale features\n",
    "\n",
    "* **Faster convergence**: unscaled features can lead to slow convergence since large features values dominate the optimization steps\n",
    "\n",
    "* **Model performance**: scaling is particularly critical for clustering algorithms, like K-means, and k-nearest neighbors, which depend on the distance between points. On the other hand, Tree-based algorithms are less sensitive to feature scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will implement some of the most important scalers.\n",
    "The construction will be based on classes, each composed of the following functions:\n",
    "1. __init__: \n",
    "2. fit\n",
    "3. transform\n",
    "4. inverse_transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# MinMax Scaler\n",
    "\n",
    "#class MinMaxScaler:\n",
    "class MinMaxScaler:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor initialized the object, setting X_min and X_max as \n",
    "        attributes\n",
    "        \"\"\"\n",
    "        self.X_min = None\n",
    "        self.X_max = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        The fit method calculates the min and max values from the data.\n",
    "        \"\"\"\n",
    "        # Guarante that X is an numpy array\n",
    "        X = np.array(X)\n",
    "\n",
    "        # Compute min/max values of X\n",
    "        self.X_min = min(X)\n",
    "        self.X_max = max(X)\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        The transform method scales the data using X_min and X_max and applying \n",
    "        the formula X_scaled = (x-min)/(max-min)\n",
    "        \"\"\"\n",
    "        if self.X_min == None or self.X_max == None:\n",
    "            raise ValueError(\"Scaller not fitted yet. Please call fit\")\n",
    "\n",
    "        if self.X_min == self.X_max:\n",
    "            raise ValueError(\"The minimum and maximum value in the data are the same. Please, check and fit again\")\n",
    "\n",
    "        X = np.array(X)\n",
    "        X_scaled = (X - self.X_min)/(self.X_max - self.X_min)\n",
    "\n",
    "        return X_scaled\n",
    "\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"\n",
    "        The inverse_transform method reverses the scaling process.\n",
    "        \"\"\"\n",
    "        X_original = X_scaled* (self.X_max - self.X_min) + self.X_min\n",
    "\n",
    "        return X_original\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data: [0.   0.25 0.5  0.75 1.  ]\n",
      "Original data: [10. 20. 30. 40. 50.]\n"
     ]
    }
   ],
   "source": [
    "X = [10, 20, 30, 40, 50]\n",
    "a = MinMaxScaler()\n",
    "a.fit(X)\n",
    "\n",
    "# Transform the data (scale it)\n",
    "X_scaled = a.transform(X)\n",
    "print(\"Scaled data:\", X_scaled)\n",
    "\n",
    "# Inverse transform the scaled data (return to original scale)\n",
    "original_data = a.inverse_transform(X_scaled)\n",
    "print(\"Original data:\", original_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler\n",
    "\n",
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.X_mean = None\n",
    "        self.X_std = None\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = np.array(X)\n",
    "\n",
    "        self.X_mean = np.mean(X)\n",
    "        self.X_std = np.std(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        if self.X_mean == None or self.X_std ==None:\n",
    "            raise ValueError(\"Please fit your data before transforming it\")\n",
    "        \n",
    "        if self.X_std == 0:\n",
    "            raise ValueError(\"There is no variability in the data.\")\n",
    "\n",
    "        X = np.array(X)\n",
    "        X_standard = (X - self.X_mean)/self.X_std\n",
    "        return X_standard\n",
    "\n",
    "    def inverse_transform(self, X_standard):\n",
    "        X_original = X_standard*self.X_std + self.X_mean\n",
    "        return X_original\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data: [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n",
      "Original data: [10. 20. 30. 40. 50.]\n"
     ]
    }
   ],
   "source": [
    "X = [10, 20, 30, 40, 50]\n",
    "a = StandardScaler()\n",
    "a.fit(X)\n",
    "\n",
    "# Transform the data (scale it)\n",
    "X_scaled = a.transform(X)\n",
    "print(\"Scaled data:\", X_scaled)\n",
    "\n",
    "# Inverse transform the scaled data (return to original scale)\n",
    "original_data = a.inverse_transform(X_scaled)\n",
    "print(\"Original data:\", original_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Scaler\n",
    "\n",
    "The Robust Scaler uses the **median** and the **interquartile range (IQR)*** \n",
    "for scaling, which makes it **robust to outliers**:\n",
    "\n",
    "$$ X_{\\text{scaled}} = \\frac{X - median(X)}{IQR(R)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustScaler:\n",
    "    def __init__(self):\n",
    "        self.X_median = None\n",
    "        self.X_IQR = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        X = np.array(X)\n",
    "        self.X_median = np.median(X)\n",
    "        self.X_IQR = np.percentile(X, 75) - np.percentile(X,25)\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = np.array(X) \n",
    "        if self.X_median == None or self.X_IQR == None:\n",
    "            raise ValueError(\"Please fit your data before transforming it.\")\n",
    "\n",
    "        if self.X_IQR == 0:\n",
    "            raise ValueError(\"The values of the 75 and 25 percentiles are the same. Please check your data and fit it again\")\n",
    "        X_scaled = (X - self.X_median)/self.X_IQR\n",
    "        return X_scaled\n",
    "\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        X_original = (X_scaled * self.X_IQR) + self.X_median\n",
    "        return X_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data: [-1.  -0.5  0.   0.5  1. ]\n",
      "Original data: [10. 20. 30. 40. 50.]\n"
     ]
    }
   ],
   "source": [
    "X = [10, 20, 30, 40, 50]\n",
    "\n",
    "a = RobustScaler()\n",
    "a.fit(X)\n",
    "\n",
    "# Transform the data (scale it)\n",
    "X_scaled = a.transform(X)\n",
    "print(\"Scaled data:\", X_scaled)\n",
    "\n",
    "# Inverse transform the scaled data (return to original scale)\n",
    "original_data = a.inverse_transform(X_scaled)\n",
    "print(\"Original data:\", original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Choosing the Right Scaling Method:\n",
    "\n",
    "1.\tStandardization (Z-Score Scaling):\n",
    "* When to use: When you donâ€™t have extreme outliers, and gradient-based\n",
    "     optimization is used (e.g., logistic regression, neural networks).\n",
    "* Pros: Works well for most machine learning algorithms, especially if the\n",
    "     features are normally distributed or have similar ranges.\n",
    "* Cons: Sensitive to outliers, which can skew the mean and standard deviation.\n",
    "* Note: standardization does not requires features to be normally distributed.\n",
    "\n",
    "\n",
    "2.\tMin-Max Scaling:\n",
    "* When to use: When you need features to be within a bounded range (e.g., 0 to 1) or when the algorithm is sensitive to feature magnitudes (e.g., distance-based algorithms like KNN, SVMs).\n",
    "* Pros: Ensures all features are within a fixed range, useful when the algorithm expects input features in a specific range.\n",
    "* Cons: Very sensitive to outliers, as they can affect the minimum and maximum values.\n",
    "\n",
    "3.\tRobust Scaling:\n",
    "* When to use: When your data has significant outliers or is not normally distributed.\n",
    "* Pros: More robust to outliers than standardization or min-max scaling.\n",
    "* Cons: Does not strictly bound the values like min-max scaling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
