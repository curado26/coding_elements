{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "Feature scaling is a crucial step in the preprocessing pipeline for many \n",
    "statistical modeling and machine learning algorithms. Scalers transform features \n",
    "so that they are on the same scale, making it easier for the models to converge \n",
    "and perform optimally.\n",
    "\n",
    "Importance:\n",
    "\n",
    "* **Consistency** across features: without scaling , algorithms may become biased toward the larger-scale features\n",
    "\n",
    "* **Faster convergence**: unscaled features can lead to slow convergence since large features values dominate the optimization steps\n",
    "\n",
    "* **Model performance**: scaling is particularly critical for clustering algorithms, like K-means, and k-nearest neighbors, which depend on the distance between points. On the other hand, Tree-based algorithms are less sensitive to feature scaling.\n",
    "\n",
    "Notice that If a feature has a variance that is orders of magnitude larger than\n",
    " others, it might dominate the objective function and make the estimator unable\n",
    "  to learn from other features correctly as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different impacts Scaling\n",
    "\n",
    "### 1. No Effect\n",
    "\n",
    "* Decision trees and everything that descends from them (random forest, boosting) are not affected by scaling. Decision trees will pick the best-split point for each variable, which is not affected by the scale of the variable.\n",
    "\n",
    "### 2. Big Effect\n",
    "\n",
    "* KNN. The **distance** measures (like Euclidian) will be affected by the unit of the variable.  \n",
    "\n",
    "### 3. Interpretability\n",
    "\n",
    "* OLS. The coefficients will be impacted by the scale of the variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will implement some of the most important scalers.\n",
    "The construction will be based on classes, each composed of the following functions:\n",
    "1. __init__: \n",
    "2. fit\n",
    "3. transform\n",
    "4. inverse_transform\n",
    "\n",
    "Like other estimators, these are represented by classes with a fit method, \n",
    "which learns model parameters (e.g. mean and standard deviation for normalization)\n",
    " from a training set, and a transform method which applies this transformation model \n",
    " (either to the training data  or to unseen data.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMax Scaler\n",
    "\n",
    "$$ X_{\\text{scaled}} = \\frac{X - x_{\\text{min}}}{ X_{\\text{max}} - X_{\\text{min}}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# MinMax Scaler\n",
    "\n",
    "#class MinMaxScaler:\n",
    "class MinMaxScaler:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor initialized the object, setting X_min and X_max as \n",
    "        attributes\n",
    "        \"\"\"\n",
    "        self.X_min = None\n",
    "        self.X_max = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        The fit method calculates the min and max values from the data.\n",
    "        \"\"\"\n",
    "        # Guarante that X is an numpy array\n",
    "        X = np.array(X)\n",
    "\n",
    "        # Compute min/max values of X\n",
    "        self.X_min = min(X)\n",
    "        self.X_max = max(X)\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        The transform method scales the data using X_min and X_max and applying \n",
    "        the formula X_scaled = (x-min)/(max-min)\n",
    "        \"\"\"\n",
    "        if self.X_min == None or self.X_max == None:\n",
    "            raise ValueError(\"Scaller not fitted yet. Please call fit\")\n",
    "\n",
    "        if self.X_min == self.X_max:\n",
    "            raise ValueError(\"The minimum and maximum value in the data are the same. Please, check and fit again\")\n",
    "\n",
    "        X = np.array(X)\n",
    "        X_scaled = (X - self.X_min)/(self.X_max - self.X_min)\n",
    "\n",
    "        return X_scaled\n",
    "\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"\n",
    "        The inverse_transform method reverses the scaling process.\n",
    "        \"\"\"\n",
    "        X_original = X_scaled* (self.X_max - self.X_min) + self.X_min\n",
    "\n",
    "        return X_original\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[10. 20. 30. 40. 50.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m50\u001b[39m]\n\u001b[1;32m      4\u001b[0m a \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[0;32m----> 5\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Transform the data (scale it)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/repos/coding_elements/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/coding_elements/.venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/coding_elements/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[0;32m~/repos/coding_elements/.venv/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/repos/coding_elements/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1050\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1045\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1046\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1048\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m             )\n\u001b[0;32m-> 1050\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[10. 20. 30. 40. 50.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = [10, 20, 30, 40, 50]\n",
    "a = MinMaxScaler()\n",
    "a.fit(X)\n",
    "\n",
    "# Transform the data (scale it)\n",
    "X_scaled = a.transform(X)\n",
    "print(\"Scaled data:\", X_scaled)\n",
    "\n",
    "# Inverse transform the scaled data (return to original scale)\n",
    "original_data = a.inverse_transform(X_scaled)\n",
    "print(\"Original data:\", original_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler\n",
    "\n",
    "$$ X_{\\text{scaled}} = \\frac{X - mean(X)}{std(X)}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler\n",
    "\n",
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.X_mean = None\n",
    "        self.X_std = None\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = np.array(X)\n",
    "\n",
    "        self.X_mean = np.mean(X)\n",
    "        self.X_std = np.std(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        if self.X_mean == None or self.X_std ==None:\n",
    "            raise ValueError(\"Please fit your data before transforming it\")\n",
    "        \n",
    "        if self.X_std == 0:\n",
    "            raise ValueError(\"There is no variability in the data.\")\n",
    "\n",
    "        X = np.array(X)\n",
    "        X_standard = (X - self.X_mean)/self.X_std\n",
    "        return X_standard\n",
    "\n",
    "    def inverse_transform(self, X_standard):\n",
    "        X_original = X_standard*self.X_std + self.X_mean\n",
    "        return X_original\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data: [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n",
      "Original data: [10. 20. 30. 40. 50.]\n"
     ]
    }
   ],
   "source": [
    "X = [10, 20, 30, 40, 50]\n",
    "a = StandardScaler()\n",
    "a.fit(X)\n",
    "\n",
    "# Transform the data (scale it)\n",
    "X_scaled = a.transform(X)\n",
    "print(\"Scaled data:\", X_scaled)\n",
    "\n",
    "# Inverse transform the scaled data (return to original scale)\n",
    "original_data = a.inverse_transform(X_scaled)\n",
    "print(\"Original data:\", original_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Scaler\n",
    "\n",
    "The Robust Scaler uses the **median** and the **interquartile range (IQR)*** \n",
    "for scaling, which makes it **robust to outliers**:\n",
    "\n",
    "$$ X_{\\text{scaled}} = \\frac{X - median(X)}{IQR(X)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustScaler:\n",
    "    def __init__(self):\n",
    "        self.X_median = None\n",
    "        self.X_IQR = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        X = np.array(X)\n",
    "        self.X_median = np.median(X)\n",
    "        self.X_IQR = np.percentile(X, 75) - np.percentile(X,25)\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = np.array(X) \n",
    "        if self.X_median == None or self.X_IQR == None:\n",
    "            raise ValueError(\"Please fit your data before transforming it.\")\n",
    "\n",
    "        if self.X_IQR == 0:\n",
    "            raise ValueError(\"The values of the 75 and 25 percentiles are the same. Please check your data and fit it again\")\n",
    "        X_scaled = (X - self.X_median)/self.X_IQR\n",
    "        return X_scaled\n",
    "\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        X_original = (X_scaled * self.X_IQR) + self.X_median\n",
    "        return X_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data: [-1.  -0.5  0.   0.5  1. ]\n",
      "Original data: [10. 20. 30. 40. 50.]\n"
     ]
    }
   ],
   "source": [
    "X = [10, 20, 30, 40, 50]\n",
    "\n",
    "a = RobustScaler()\n",
    "a.fit(X)\n",
    "\n",
    "# Transform the data (scale it)\n",
    "X_scaled = a.transform(X)\n",
    "print(\"Scaled data:\", X_scaled)\n",
    "\n",
    "# Inverse transform the scaled data (return to original scale)\n",
    "original_data = a.inverse_transform(X_scaled)\n",
    "print(\"Original data:\", original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Choosing the Right Scaling Method:\n",
    "\n",
    "1.\tStandardization (Z-Score Scaling):\n",
    "* When to use: When you donâ€™t have extreme outliers, and gradient-based\n",
    "     optimization is used (e.g., logistic regression, neural networks).\n",
    "* Pros: Works well for most machine learning algorithms, especially if the\n",
    "     features are normally distributed or have similar ranges.\n",
    "* Cons: Sensitive to outliers, which can skew the mean and standard deviation.\n",
    "* Note: standardization does not requires features to be normally distributed.\n",
    "\n",
    "\n",
    "2.\tMin-Max Scaling:\n",
    "* When to use: When you need features to be within a bounded range (e.g., 0 to 1) or when the algorithm is sensitive to feature magnitudes (e.g., distance-based algorithms like KNN, SVMs).\n",
    "* Pros: Ensures all features are within a fixed range, useful when the algorithm expects input features in a specific range.\n",
    "* Cons: Very sensitive to outliers, as they can affect the minimum and maximum values.\n",
    "\n",
    "3.\tRobust Scaling:\n",
    "* When to use: When your data has significant outliers or is not normally distributed.\n",
    "* Pros: More robust to outliers than standardization or min-max scaling.\n",
    "* Cons: Does not strictly bound the values like min-max scaling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
