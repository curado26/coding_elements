{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "\n",
    "Question: Write code to implement linear regression from scratch without using\n",
    "any machine learning libraries like scikit-learn.\n",
    "\n",
    "Answer Guidance: Be prepared to:\n",
    "    *\tHandle data input and preprocessing.\n",
    "\t*\tCompute the OLS estimates manually.\n",
    "\t*\tImplement matrix operations using libraries like NumPy.\n",
    "\t*\tOptionally, write functions to predict new data points and calculate metrics like Mean Squared Error (MSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/df_regression.csv') \n",
    "X = df[['X1','X2','X3']]\n",
    "y = df['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.19994425, 3.08318585, 1.52085497, 2.13173927])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generalizing for Multiple Linear Regression\n",
    "\n",
    "def multiple_linear_regression(X,y):\n",
    "\n",
    "    \"\"\"\n",
    "    Estimates the coefficients for a multiple linear regression model using the normal equation.\n",
    "\n",
    "    This function computes the coefficients (weights) for a multiple linear regression model\n",
    "    by solving the normal equation: beta = (X^T * X)^(-1) * X^T * y. It works for any number\n",
    "    of predictors (independent variables) and includes an intercept term.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : array-like or list of lists\n",
    "        Feature matrix containing the independent variables.\n",
    "        Shape: (n_samples, n_features)\n",
    "    \n",
    "    y : array-like or list\n",
    "        Target vector containing the dependent variable.\n",
    "        Shape: (n_samples,)\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    beta : numpy.ndarray\n",
    "        Coefficient vector including the intercept term and coefficients for each predictor.\n",
    "        Shape: (n_features + 1,)\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Conver inout to numpy (in case they are dataframes..)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Add intercept\n",
    "    ones = np.ones((X.shape[0], 1))  # vector of ones\n",
    "    X = np.concatenate((ones, X), axis =1)\n",
    "\n",
    "    #Compute the elements of the normal equations\n",
    "    #     A = (X'X)**(-1) \n",
    "    #     B = X'Y\n",
    "    #  beta = A* B is then the coefficient vector\n",
    "\n",
    "    A = np.linalg.inv(( X.T.dot(X)))\n",
    "    B = X.T.dot(y)\n",
    "\n",
    "    beta = A.dot(B)\n",
    "\n",
    "    return(beta)\n",
    "\n",
    "\n",
    "multiple_linear_regression(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In NumPy, the * operator performs element-wise multiplication, not matrix multiplication.\n",
    "To perform matrix multiplication, use the .dot() method or the @ operator.\n",
    "\n",
    "\n",
    "`A.dot(B)`:  Correct way to perform matrix multiplication in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_new, beta):\n",
    "    \n",
    "    # Convert input to numpy array\n",
    "    X_new = np.array(X_new)\n",
    "\n",
    "    # Add intercept term\n",
    "    ones = np.ones((X_new.shape[0]),1 )\n",
    "    X_new = np.concat((ones, X_new), axis = 1)\n",
    "\n",
    "    # Calculate prediction \n",
    "\n",
    "    y_pred = X_new.dot(beta)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible follow up question would be **How would you implement linear \n",
    "regression using gradient descent instead of the normal equation?** \n",
    "\n",
    "This gradient approach is especially useful when dealing with a large number of \n",
    "features or when the matrix inversion in the normal equation becomes computationally \n",
    "expensive. Gradient descent works by iteratively adjusting the coefficients to minimize the cost function, typically the Mean Squared Error (MSE) for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts of Gradient Descent for Linear Regression\n",
    "\n",
    "#### 3. Cost Function\n",
    "The cost function measures how well the model's predictions match the actual data. For linear regression, the cost function is the Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ m $ is the number of training examples.\n",
    "- $ h_{\\theta}(x^{(i)}) $ is the predicted value for the $ i $-th sample.\n",
    "- $ y^{(i)} $ is the actual value for the $ i $-th sample.\n",
    "\n",
    "The goal of gradient descent is to find the values of $ \\theta $ that minimize $ J(\\theta) \\).\n",
    "\n",
    "#### 4. Gradient Descent Algorithm\n",
    "Gradient Descent is an iterative optimization algorithm that updates the coefficients in the direction of the negative gradient of the cost function:\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\theta_j $ is the $ j $-th parameter.\n",
    "- $ \\alpha $ is the learning rate, controlling the step size.\n",
    "- $ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} $ is the partial derivative of the cost function with respect to $ \\theta_j $.\n",
    "\n",
    "#### 5. Gradient Calculation\n",
    "For linear regression, the gradient of the cost function with respect to each parameter $ \\theta_j $ is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ x_j^{(i)} $ is the value of the $ j $-th feature for the $ i $-th sample.\n",
    "\n",
    "#### 6. Update Rule\n",
    "The update rule for each coefficient in the gradient descent algorithm is:\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "This update rule is applied iteratively for all coefficients until convergence.\n",
    "\n",
    "#### 7. Learning Rate\n",
    "The learning rate $ \\alpha $ controls the size of the steps taken towards the minimum of the cost function. Choosing the right learning rate is crucial:\n",
    "- **Too small:** The algorithm will take too long to converge.\n",
    "- **Too large:** The algorithm may overshoot the minimum or even diverge.\n",
    "\n",
    "#### 8. Convergence\n",
    "The gradient descent algorithm converges when the cost function $ J(\\theta) $ reaches its minimum value. This can be monitored by checking the change in cost over iterations or by setting a maximum number of iterations.\n",
    "\n",
    "#### 9. Variants of Gradient Descent\n",
    "- **Batch Gradient Descent:** Uses all training examples to compute the gradient.\n",
    "- **Stochastic Gradient Descent (SGD):** Uses one training example at a time to compute the gradient, which can be faster but introduces more noise in the updates.\n",
    "- **Mini-batch Gradient Descent:** Uses a small subset (mini-batch) of the training examples to compute the gradient, balancing the efficiency and stability of the updates.\n",
    "\n",
    "#### 10. Summary\n",
    "Gradient Descent is a powerful algorithm for optimizing linear regression models, especially when dealing with large datasets. By iteratively updating the coefficients in the direction of the negative gradient of the cost function, it finds the optimal parameters that minimize the error between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.67238675, 3.26813627, 1.63272398, 2.60832671]),\n",
       " [np.float64(1063.5530721022635),\n",
       "  np.float64(380.5420589777542),\n",
       "  np.float64(152.31498412972596),\n",
       "  np.float64(72.75486483283463),\n",
       "  np.float64(42.618463147511385),\n",
       "  np.float64(29.521198359447112),\n",
       "  np.float64(22.73980593416489),\n",
       "  np.float64(18.608818921261634),\n",
       "  np.float64(15.79191578454744),\n",
       "  np.float64(13.743207205703841),\n",
       "  np.float64(12.20092152785298),\n",
       "  np.float64(11.016655709441636),\n",
       "  np.float64(10.094764250172247),\n",
       "  np.float64(9.368686325135352),\n",
       "  np.float64(8.790239051909047),\n",
       "  np.float64(8.323876102542217),\n",
       "  np.float64(7.943131149684714),\n",
       "  np.float64(7.62820049126274),\n",
       "  np.float64(7.364216757926175),\n",
       "  np.float64(7.139986627613239),\n",
       "  np.float64(6.947057820037653),\n",
       "  np.float64(6.779026115515249),\n",
       "  np.float64(6.631019492241544),\n",
       "  np.float64(6.499313677520949),\n",
       "  np.float64(6.381045439004445),\n",
       "  np.float64(6.273998651046979),\n",
       "  np.float64(6.176444577226952),\n",
       "  np.float64(6.087022555597802),\n",
       "  np.float64(6.004650799747862),\n",
       "  np.float64(5.928459652952532),\n",
       "  np.float64(5.85774158664799),\n",
       "  np.float64(5.791913689722748),\n",
       "  np.float64(5.73048947911979),\n",
       "  np.float64(5.673057669741658),\n",
       "  np.float64(5.619266143206581),\n",
       "  np.float64(5.568809803157391),\n",
       "  np.float64(5.5214213387092),\n",
       "  np.float64(5.476864166385037),\n",
       "  np.float64(5.434927006243217),\n",
       "  np.float64(5.395419686018771),\n",
       "  np.float64(5.358169870031726),\n",
       "  np.float64(5.323020486330605),\n",
       "  np.float64(5.289827682725884),\n",
       "  np.float64(5.258459185004189),\n",
       "  np.float64(5.228792962409667),\n",
       "  np.float64(5.20071612919708),\n",
       "  np.float64(5.174124028760537),\n",
       "  np.float64(5.14891946005557),\n",
       "  np.float64(5.1250120159030566),\n",
       "  np.float64(5.102317510142155),\n",
       "  np.float64(5.080757476120336),\n",
       "  np.float64(5.060258723143715),\n",
       "  np.float64(5.04075294061274),\n",
       "  np.float64(5.022176341898642),\n",
       "  np.float64(5.004469341770793),\n",
       "  np.float64(4.987576262509678),\n",
       "  np.float64(4.9714450648432615),\n",
       "  np.float64(4.95602710060701),\n",
       "  np.float64(4.941276884610247),\n",
       "  np.float64(4.927151883638714),\n",
       "  np.float64(4.91361232086887),\n",
       "  np.float64(4.900620994238785),\n",
       "  np.float64(4.888143107532086),\n",
       "  np.float64(4.876146113099154),\n",
       "  np.float64(4.864599565274686),\n",
       "  np.float64(4.853474983659843),\n",
       "  np.float64(4.842745725527273),\n",
       "  np.float64(4.832386866681976),\n",
       "  np.float64(4.822375090174169),\n",
       "  np.float64(4.81268858231423),\n",
       "  np.float64(4.803306935486362),\n",
       "  np.float64(4.794211057298416),\n",
       "  np.float64(4.785383085641311),\n",
       "  np.float64(4.776806309263669),\n",
       "  np.float64(4.768465093495958),\n",
       "  np.float64(4.760344810784846),\n",
       "  np.float64(4.752431775721986),\n",
       "  np.float64(4.744713184273402),\n",
       "  np.float64(4.7371770569355816),\n",
       "  np.float64(4.729812185562789),\n",
       "  np.float64(4.722608083627289),\n",
       "  np.float64(4.715554939689841),\n",
       "  np.float64(4.7086435738725605),\n",
       "  np.float64(4.701865397139911),\n",
       "  np.float64(4.695212373206239),\n",
       "  np.float64(4.688676982900183),\n",
       "  np.float64(4.682252190827189),\n",
       "  np.float64(4.675931414181882),\n",
       "  np.float64(4.669708493571419),\n",
       "  np.float64(4.663577665720141),\n",
       "  np.float64(4.657533537934128),\n",
       "  np.float64(4.651571064212138),\n",
       "  np.float64(4.6456855228967),\n",
       "  np.float64(4.63987249576612),\n",
       "  np.float64(4.63412784847442),\n",
       "  np.float64(4.628447712252304),\n",
       "  np.float64(4.622828466787827),\n",
       "  np.float64(4.617266724210763),\n",
       "  np.float64(4.611759314109436),\n",
       "  np.float64(4.606303269513488),\n",
       "  np.float64(4.600895813780315),\n",
       "  np.float64(4.595534348326892),\n",
       "  np.float64(4.59021644115249),\n",
       "  np.float64(4.584939816101298),\n",
       "  np.float64(4.579702342817259),\n",
       "  np.float64(4.574502027346465),\n",
       "  np.float64(4.569337003345415),\n",
       "  np.float64(4.56420552385601),\n",
       "  np.float64(4.55910595361082),\n",
       "  np.float64(4.554036761834383),\n",
       "  np.float64(4.548996515508573),\n",
       "  np.float64(4.543983873072156),\n",
       "  np.float64(4.538997578526468),\n",
       "  np.float64(4.534036455921124),\n",
       "  np.float64(4.529099404195177),\n",
       "  np.float64(4.524185392350864),\n",
       "  np.float64(4.519293454938482),\n",
       "  np.float64(4.514422687832334),\n",
       "  np.float64(4.509572244278981),\n",
       "  np.float64(4.50474133120025),\n",
       "  np.float64(4.499929205734574),\n",
       "  np.float64(4.49513517200129),\n",
       "  np.float64(4.490358578073544),\n",
       "  np.float64(4.485598813146317),\n",
       "  np.float64(4.480855304887045),\n",
       "  np.float64(4.4761275169570105),\n",
       "  np.float64(4.4714149466925175),\n",
       "  np.float64(4.4667171229355835),\n",
       "  np.float64(4.462033604004433),\n",
       "  np.float64(4.457363975794846),\n",
       "  np.float64(4.452707850003911),\n",
       "  np.float64(4.448064862468268),\n",
       "  np.float64(4.44343467160948),\n",
       "  np.float64(4.438816956979634),\n",
       "  np.float64(4.434211417900698),\n",
       "  np.float64(4.429617772191563),\n",
       "  np.float64(4.42503575497719),\n",
       "  np.float64(4.420465117574504),\n",
       "  np.float64(4.415905626450082),\n",
       "  np.float64(4.41135706224509),\n",
       "  np.float64(4.406819218863028),\n",
       "  np.float64(4.402291902616287),\n",
       "  np.float64(4.3977749314277625),\n",
       "  np.float64(4.393268134083887),\n",
       "  np.float64(4.388771349535827),\n",
       "  np.float64(4.384284426245754),\n",
       "  np.float64(4.379807221575207),\n",
       "  np.float64(4.375339601212945),\n",
       "  np.float64(4.370881438639625),\n",
       "  np.float64(4.366432614627001),\n",
       "  np.float64(4.3619930167694445),\n",
       "  np.float64(4.357562539045604),\n",
       "  np.float64(4.3531410814083715),\n",
       "  np.float64(4.34872854940125),\n",
       "  np.float64(4.344324853799442),\n",
       "  np.float64(4.339929910274121),\n",
       "  np.float64(4.335543639078285),\n",
       "  np.float64(4.331165964752901),\n",
       "  np.float64(4.326796815851961),\n",
       "  np.float64(4.322436124685299),\n",
       "  np.float64(4.31808382707793),\n",
       "  np.float64(4.313739862144959),\n",
       "  np.float64(4.309404172080961),\n",
       "  np.float64(4.305076701962933),\n",
       "  np.float64(4.300757399565994),\n",
       "  np.float64(4.296446215190884),\n",
       "  np.float64(4.292143101502677),\n",
       "  np.float64(4.287848013379789),\n",
       "  np.float64(4.283560907772807),\n",
       "  np.float64(4.279281743572353),\n",
       "  np.float64(4.275010481485496),\n",
       "  np.float64(4.270747083920115),\n",
       "  np.float64(4.266491514876722),\n",
       "  np.float64(4.262243739847261),\n",
       "  np.float64(4.258003725720395),\n",
       "  np.float64(4.2537714406929545),\n",
       "  np.float64(4.249546854187035),\n",
       "  np.float64(4.245329936772476),\n",
       "  np.float64(4.241120660094307),\n",
       "  np.float64(4.23691899680489),\n",
       "  np.float64(4.2327249205004245),\n",
       "  np.float64(4.228538405661539),\n",
       "  np.float64(4.22435942759772),\n",
       "  np.float64(4.220187962395311),\n",
       "  np.float64(4.216023986868869),\n",
       "  np.float64(4.211867478515648),\n",
       "  np.float64(4.207718415473026),\n",
       "  np.float64(4.203576776478652),\n",
       "  np.float64(4.199442540833181),\n",
       "  np.float64(4.195315688365408),\n",
       "  np.float64(4.191196199399624),\n",
       "  np.float64(4.187084054725109),\n",
       "  np.float64(4.182979235567545),\n",
       "  np.float64(4.178881723562324),\n",
       "  np.float64(4.1747915007295315),\n",
       "  np.float64(4.170708549450547),\n",
       "  np.float64(4.166632852446161),\n",
       "  np.float64(4.1625643927561145),\n",
       "  np.float64(4.158503153719905),\n",
       "  np.float64(4.15444911895889),\n",
       "  np.float64(4.150402272359493),\n",
       "  np.float64(4.14636259805751),\n",
       "  np.float64(4.142330080423434),\n",
       "  np.float64(4.1383047040487),\n",
       "  np.float64(4.134286453732836),\n",
       "  np.float64(4.1302753144714135),\n",
       "  np.float64(4.126271271444822),\n",
       "  np.float64(4.1222743100077),\n",
       "  np.float64(4.118284415679088),\n",
       "  np.float64(4.114301574133216),\n",
       "  np.float64(4.110325771190843),\n",
       "  np.float64(4.106356992811208),\n",
       "  np.float64(4.102395225084445),\n",
       "  np.float64(4.098440454224521),\n",
       "  np.float64(4.094492666562613),\n",
       "  np.float64(4.090551848540901),\n",
       "  np.float64(4.086617986706775),\n",
       "  np.float64(4.082691067707417),\n",
       "  np.float64(4.078771078284696),\n",
       "  np.float64(4.074858005270429),\n",
       "  np.float64(4.070951835581921),\n",
       "  np.float64(4.067052556217812),\n",
       "  np.float64(4.063160154254155),\n",
       "  np.float64(4.0592746168407805),\n",
       "  np.float64(4.055395931197869),\n",
       "  np.float64(4.051524084612758),\n",
       "  np.float64(4.047659064436937),\n",
       "  np.float64(4.043800858083252),\n",
       "  np.float64(4.03994945302326),\n",
       "  np.float64(4.036104836784793),\n",
       "  np.float64(4.032266996949638),\n",
       "  np.float64(4.028435921151376),\n",
       "  np.float64(4.024611597073377),\n",
       "  np.float64(4.0207940124468955),\n",
       "  np.float64(4.016983155049296),\n",
       "  np.float64(4.013179012702398),\n",
       "  np.float64(4.009381573270919),\n",
       "  np.float64(4.005590824661007),\n",
       "  np.float64(4.001806754818883),\n",
       "  np.float64(3.9980293517295658),\n",
       "  np.float64(3.9942586034156546),\n",
       "  np.float64(3.990494497936209),\n",
       "  np.float64(3.9867370233857065),\n",
       "  np.float64(3.982986167893032),\n",
       "  np.float64(3.979241919620573),\n",
       "  np.float64(3.975504266763327),\n",
       "  np.float64(3.9717731975480985),\n",
       "  np.float64(3.9680487002327327),\n",
       "  np.float64(3.964330763105386),\n",
       "  np.float64(3.9606193744838625),\n",
       "  np.float64(3.9569145227149796),\n",
       "  np.float64(3.9532161961739583),\n",
       "  np.float64(3.949524383263879),\n",
       "  np.float64(3.94583907241516),\n",
       "  np.float64(3.9421602520850474),\n",
       "  np.float64(3.938487910757163),\n",
       "  np.float64(3.93482203694106),\n",
       "  np.float64(3.9311626191718285),\n",
       "  np.float64(3.927509646009684),\n",
       "  np.float64(3.92386310603963),\n",
       "  np.float64(3.920222987871097),\n",
       "  np.float64(3.9165892801376287),\n",
       "  np.float64(3.9129619714965767),\n",
       "  np.float64(3.909341050628811),\n",
       "  np.float64(3.905726506238454),\n",
       "  np.float64(3.902118327052632),\n",
       "  np.float64(3.898516501821212),\n",
       "  np.float64(3.8949210193165973),\n",
       "  np.float64(3.8913318683335043),\n",
       "  np.float64(3.8877490376887556),\n",
       "  np.float64(3.884172516221093),\n",
       "  np.float64(3.8806022927909933),\n",
       "  np.float64(3.8770383562804978),\n",
       "  np.float64(3.873480695593043),\n",
       "  np.float64(3.869929299653315),\n",
       "  np.float64(3.866384157407091),\n",
       "  np.float64(3.8628452578211037),\n",
       "  np.float64(3.859312589882922),\n",
       "  np.float64(3.855786142600792),\n",
       "  np.float64(3.8522659050035544),\n",
       "  np.float64(3.8487518661405016),\n",
       "  np.float64(3.8452440150812777),\n",
       "  np.float64(3.841742340915778),\n",
       "  np.float64(3.8382468327540384),\n",
       "  np.float64(3.834757479726154),\n",
       "  np.float64(3.8312742709821688),\n",
       "  np.float64(3.827797195692007),\n",
       "  np.float64(3.824326243045373),\n",
       "  np.float64(3.8208614022516834),\n",
       "  np.float64(3.817402662539974),\n",
       "  np.float64(3.813950013158843),\n",
       "  np.float64(3.8105034433763594),\n",
       "  np.float64(3.80706294248001),\n",
       "  np.float64(3.803628499776625),\n",
       "  np.float64(3.8002001045923066),\n",
       "  np.float64(3.7967777462723813),\n",
       "  np.float64(3.79336141418132),\n",
       "  np.float64(3.789951097702702),\n",
       "  np.float64(3.7865467862391275),\n",
       "  np.float64(3.7831484692121964),\n",
       "  np.float64(3.779756136062423),\n",
       "  np.float64(3.7763697762492106),\n",
       "  np.float64(3.772989379250768),\n",
       "  np.float64(3.769614934564101),\n",
       "  np.float64(3.7662464317049182),\n",
       "  np.float64(3.762883860207619),\n",
       "  np.float64(3.759527209625227),\n",
       "  np.float64(3.7561764695293527),\n",
       "  np.float64(3.752831629510146),\n",
       "  np.float64(3.749492679176246),\n",
       "  np.float64(3.7461596081547537),\n",
       "  np.float64(3.7428324060911757),\n",
       "  np.float64(3.7395110626493837),\n",
       "  np.float64(3.7361955675115786),\n",
       "  np.float64(3.7328859103782506),\n",
       "  np.float64(3.7295820809681293),\n",
       "  np.float64(3.7262840690181527),\n",
       "  np.float64(3.722991864283439),\n",
       "  np.float64(3.7197054565372203),\n",
       "  np.float64(3.716424835570831),\n",
       "  np.float64(3.7131499911936565),\n",
       "  np.float64(3.7098809132331065),\n",
       "  np.float64(3.706617591534563),\n",
       "  np.float64(3.703360015961362),\n",
       "  np.float64(3.7001081763947434),\n",
       "  np.float64(3.696862062733822),\n",
       "  np.float64(3.6936216648955584),\n",
       "  np.float64(3.6903869728147063),\n",
       "  np.float64(3.6871579764438014),\n",
       "  np.float64(3.683934665753105),\n",
       "  np.float64(3.680717030730581),\n",
       "  np.float64(3.6775050613818685),\n",
       "  np.float64(3.674298747730231),\n",
       "  np.float64(3.671098079816541),\n",
       "  np.float64(3.66790304769923),\n",
       "  np.float64(3.6647136414542714),\n",
       "  np.float64(3.661529851175134),\n",
       "  np.float64(3.658351666972766),\n",
       "  np.float64(3.6551790789755403),\n",
       "  np.float64(3.652012077329242),\n",
       "  np.float64(3.64885065219702),\n",
       "  np.float64(3.6456947937593784),\n",
       "  np.float64(3.642544492214117),\n",
       "  np.float64(3.6393997377763196),\n",
       "  np.float64(3.6362605206783094),\n",
       "  np.float64(3.633126831169633),\n",
       "  np.float64(3.629998659517008),\n",
       "  np.float64(3.626875996004315),\n",
       "  np.float64(3.623758830932548),\n",
       "  np.float64(3.6206471546197943),\n",
       "  np.float64(3.617540957401195),\n",
       "  np.float64(3.614440229628928),\n",
       "  np.float64(3.61134496167216),\n",
       "  np.float64(3.6082551439170287),\n",
       "  np.float64(3.6051707667666086),\n",
       "  np.float64(3.6020918206408785),\n",
       "  np.float64(3.5990182959766943),\n",
       "  np.float64(3.5959501832277607),\n",
       "  np.float64(3.5928874728645894),\n",
       "  np.float64(3.589830155374488),\n",
       "  np.float64(3.5867782212615102),\n",
       "  np.float64(3.5837316610464467),\n",
       "  np.float64(3.5806904652667697),\n",
       "  np.float64(3.57765462447663),\n",
       "  np.float64(3.5746241292468084),\n",
       "  np.float64(3.5715989701647004),\n",
       "  np.float64(3.568579137834267),\n",
       "  np.float64(3.5655646228760287),\n",
       "  np.float64(3.5625554159270165),\n",
       "  np.float64(3.5595515076407582),\n",
       "  np.float64(3.5565528886872353),\n",
       "  np.float64(3.553559549752865),\n",
       "  np.float64(3.5505714815404645),\n",
       "  np.float64(3.5475886747692256),\n",
       "  np.float64(3.544611120174679),\n",
       "  np.float64(3.54163880850868),\n",
       "  np.float64(3.538671730539359),\n",
       "  np.float64(3.5357098770511106),\n",
       "  np.float64(3.5327532388445597),\n",
       "  np.float64(3.529801806736525),\n",
       "  np.float64(3.526855571560001),\n",
       "  np.float64(3.52391452416412),\n",
       "  np.float64(3.5209786554141376),\n",
       "  np.float64(3.518047956191387),\n",
       "  np.float64(3.515122417393261),\n",
       "  np.float64(3.512202029933185),\n",
       "  np.float64(3.5092867847405795),\n",
       "  np.float64(3.506376672760843),\n",
       "  np.float64(3.503471684955315),\n",
       "  np.float64(3.500571812301248),\n",
       "  np.float64(3.497677045791793),\n",
       "  np.float64(3.494787376435947),\n",
       "  np.float64(3.491902795258553),\n",
       "  np.float64(3.489023293300249),\n",
       "  np.float64(3.486148861617457),\n",
       "  np.float64(3.483279491282336),\n",
       "  np.float64(3.4804151733827773),\n",
       "  np.float64(3.4775558990223625),\n",
       "  np.float64(3.474701659320331),\n",
       "  np.float64(3.4718524454115682),\n",
       "  np.float64(3.469008248446571),\n",
       "  np.float64(3.4661690595914103),\n",
       "  np.float64(3.463334870027719),\n",
       "  np.float64(3.4605056709526583),\n",
       "  np.float64(3.4576814535788856),\n",
       "  np.float64(3.4548622091345322),\n",
       "  np.float64(3.4520479288631836),\n",
       "  np.float64(3.449238604023828),\n",
       "  np.float64(3.4464342258908607),\n",
       "  np.float64(3.443634785754033),\n",
       "  np.float64(3.4408402749184326),\n",
       "  np.float64(3.4380506847044634),\n",
       "  np.float64(3.4352660064478124),\n",
       "  np.float64(3.4324862314994165),\n",
       "  np.float64(3.4297113512254453),\n",
       "  np.float64(3.4269413570072764),\n",
       "  np.float64(3.424176240241453),\n",
       "  np.float64(3.421415992339677),\n",
       "  np.float64(3.4186606047287706),\n",
       "  np.float64(3.4159100688506463),\n",
       "  np.float64(3.413164376162293),\n",
       "  np.float64(3.410423518135737),\n",
       "  np.float64(3.4076874862580264),\n",
       "  np.float64(3.404956272031195),\n",
       "  np.float64(3.4022298669722404),\n",
       "  np.float64(3.3995082626130966),\n",
       "  np.float64(3.396791450500613),\n",
       "  np.float64(3.3940794221965196),\n",
       "  np.float64(3.3913721692774024),\n",
       "  np.float64(3.3886696833346877),\n",
       "  np.float64(3.3859719559746),\n",
       "  np.float64(3.383278978818143),\n",
       "  np.float64(3.3805907435010836),\n",
       "  np.float64(3.3779072416739058),\n",
       "  np.float64(3.375228465001805),\n",
       "  np.float64(3.372554405164643),\n",
       "  np.float64(3.3698850538569447),\n",
       "  np.float64(3.3672204027878445),\n",
       "  np.float64(3.364560443681086),\n",
       "  np.float64(3.3619051682749848),\n",
       "  np.float64(3.359254568322398),\n",
       "  np.float64(3.3566086355907117),\n",
       "  np.float64(3.3539673618617996),\n",
       "  np.float64(3.351330738932017),\n",
       "  np.float64(3.348698758612154),\n",
       "  np.float64(3.346071412727427),\n",
       "  np.float64(3.3434486931174456),\n",
       "  np.float64(3.340830591636183),\n",
       "  np.float64(3.338217100151965),\n",
       "  np.float64(3.3356082105474307),\n",
       "  np.float64(3.3330039147195114),\n",
       "  np.float64(3.330404204579405),\n",
       "  np.float64(3.3278090720525597),\n",
       "  np.float64(3.325218509078638),\n",
       "  np.float64(3.322632507611489),\n",
       "  np.float64(3.3200510596191397),\n",
       "  np.float64(3.3174741570837516),\n",
       "  np.float64(3.314901792001611),\n",
       "  np.float64(3.312333956383091),\n",
       "  np.float64(3.3097706422526376),\n",
       "  np.float64(3.3072118416487384),\n",
       "  np.float64(3.3046575466238988),\n",
       "  np.float64(3.3021077492446227),\n",
       "  np.float64(3.2995624415913762),\n",
       "  np.float64(3.2970216157585743),\n",
       "  np.float64(3.294485263854557),\n",
       "  np.float64(3.2919533780015504),\n",
       "  np.float64(3.2894259503356587),\n",
       "  np.float64(3.2869029730068333),\n",
       "  np.float64(3.2843844381788414),\n",
       "  np.float64(3.281870338029254),\n",
       "  np.float64(3.2793606647494205),\n",
       "  np.float64(3.276855410544427),\n",
       "  np.float64(3.274354567633094),\n",
       "  np.float64(3.271858128247944),\n",
       "  np.float64(3.269366084635174),\n",
       "  np.float64(3.2668784290546298),\n",
       "  np.float64(3.2643951537797977),\n",
       "  np.float64(3.2619162510977553),\n",
       "  np.float64(3.2594417133091707),\n",
       "  np.float64(3.2569715327282664),\n",
       "  np.float64(3.2545057016827963),\n",
       "  np.float64(3.252044212514025),\n",
       "  np.float64(3.2495870575767083),\n",
       "  np.float64(3.2471342292390553),\n",
       "  np.float64(3.2446857198827126),\n",
       "  np.float64(3.242241521902751),\n",
       "  np.float64(3.2398016277076254),\n",
       "  np.float64(3.237366029719157),\n",
       "  np.float64(3.2349347203725145),\n",
       "  np.float64(3.232507692116186),\n",
       "  np.float64(3.230084937411955),\n",
       "  np.float64(3.2276664487348796),\n",
       "  np.float64(3.2252522185732677),\n",
       "  np.float64(3.22284223942865),\n",
       "  np.float64(3.22043650381577),\n",
       "  np.float64(3.2180350042625454),\n",
       "  np.float64(3.2156377333100483),\n",
       "  np.float64(3.2132446835124893),\n",
       "  np.float64(3.210855847437183),\n",
       "  np.float64(3.2084712176645453),\n",
       "  np.float64(3.206090786788042),\n",
       "  np.float64(3.203714547414184),\n",
       "  np.float64(3.2013424921625124),\n",
       "  np.float64(3.198974613665545),\n",
       "  np.float64(3.196610904568788),\n",
       "  np.float64(3.1942513575306872),\n",
       "  np.float64(3.1918959652226233),\n",
       "  np.float64(3.1895447203288754),\n",
       "  np.float64(3.18719761554661),\n",
       "  np.float64(3.184854643585845),\n",
       "  np.float64(3.182515797169442),\n",
       "  np.float64(3.1801810690330723),\n",
       "  np.float64(3.1778504519251998),\n",
       "  np.float64(3.1755239386070544),\n",
       "  np.float64(3.1732015218526186),\n",
       "  np.float64(3.17088319444859),\n",
       "  np.float64(3.168568949194376),\n",
       "  np.float64(3.166258778902052),\n",
       "  np.float64(3.163952676396361),\n",
       "  np.float64(3.1616506345146758),\n",
       "  np.float64(3.1593526461069805),\n",
       "  np.float64(3.1570587040358555),\n",
       "  np.float64(3.154768801176438),\n",
       "  np.float64(3.152482930416413),\n",
       "  np.float64(3.1502010846559996),\n",
       "  np.float64(3.1479232568079043),\n",
       "  np.float64(3.1456494397973263),\n",
       "  np.float64(3.143379626561909),\n",
       "  np.float64(3.141113810051745),\n",
       "  np.float64(3.138851983229326),\n",
       "  np.float64(3.136594139069548),\n",
       "  np.float64(3.134340270559669),\n",
       "  np.float64(3.1320903706992986),\n",
       "  np.float64(3.129844432500373),\n",
       "  np.float64(3.1276024489871315),\n",
       "  np.float64(3.1253644131960994),\n",
       "  np.float64(3.123130318176057),\n",
       "  np.float64(3.120900156988032),\n",
       "  np.float64(3.118673922705263),\n",
       "  np.float64(3.116451608413196),\n",
       "  np.float64(3.114233207209437),\n",
       "  np.float64(3.112018712203765),\n",
       "  np.float64(3.109808116518069),\n",
       "  np.float64(3.107601413286368),\n",
       "  np.float64(3.1053985956547625),\n",
       "  np.float64(3.1031996567814253),\n",
       "  np.float64(3.1010045898365672),\n",
       "  np.float64(3.0988133880024367),\n",
       "  np.float64(3.0966260444732803),\n",
       "  np.float64(3.0944425524553316),\n",
       "  np.float64(3.0922629051667814),\n",
       "  np.float64(3.0900870958377675),\n",
       "  np.float64(3.0879151177103474),\n",
       "  np.float64(3.085746964038475),\n",
       "  np.float64(3.083582628087987),\n",
       "  np.float64(3.081422103136572),\n",
       "  np.float64(3.0792653824737646),\n",
       "  np.float64(3.077112459400906),\n",
       "  np.float64(3.0749633272311336),\n",
       "  np.float64(3.0728179792893666),\n",
       "  np.float64(3.0706764089122682),\n",
       "  np.float64(3.0685386094482414),\n",
       "  np.float64(3.066404574257401),\n",
       "  np.float64(3.0642742967115497),\n",
       "  np.float64(3.062147770194163),\n",
       "  np.float64(3.0600249881003654),\n",
       "  np.float64(3.0579059438369147),\n",
       "  np.float64(3.0557906308221776),\n",
       "  np.float64(3.0536790424861056),\n",
       "  np.float64(3.0515711722702283),\n",
       "  np.float64(3.0494670136276096),\n",
       "  np.float64(3.0473665600228546),\n",
       "  np.float64(3.0452698049320657),\n",
       "  np.float64(3.0431767418428395),\n",
       "  np.float64(3.041087364254238),\n",
       "  np.float64(3.0390016656767727),\n",
       "  np.float64(3.0369196396323743),\n",
       "  np.float64(3.0348412796543873),\n",
       "  np.float64(3.032766579287541),\n",
       "  np.float64(3.030695532087928),\n",
       "  np.float64(3.0286281316229924),\n",
       "  np.float64(3.026564371471502),\n",
       "  np.float64(3.02450424522353),\n",
       "  np.float64(3.022447746480441),\n",
       "  np.float64(3.0203948688548645),\n",
       "  np.float64(3.01834560597067),\n",
       "  np.float64(3.0162999514629663),\n",
       "  np.float64(3.0142578989780606),\n",
       "  np.float64(3.0122194421734503),\n",
       "  np.float64(3.010184574717802),\n",
       "  np.float64(3.008153290290931),\n",
       "  np.float64(3.0061255825837776),\n",
       "  np.float64(3.0041014452983967),\n",
       "  np.float64(3.0020808721479266),\n",
       "  np.float64(3.00006385685658),\n",
       "  np.float64(2.9980503931596214),\n",
       "  np.float64(2.9960404748033387),\n",
       "  np.float64(2.9940340955450395),\n",
       "  np.float64(2.9920312491530217),\n",
       "  np.float64(2.9900319294065536),\n",
       "  np.float64(2.988036130095858),\n",
       "  np.float64(2.986043845022092),\n",
       "  np.float64(2.984055067997332),\n",
       "  np.float64(2.9820697928445403),\n",
       "  np.float64(2.980088013397568),\n",
       "  np.float64(2.9781097235011145),\n",
       "  np.float64(2.976134917010721),\n",
       "  np.float64(2.9741635877927473),\n",
       "  np.float64(2.972195729724354),\n",
       "  np.float64(2.9702313366934834),\n",
       "  np.float64(2.9682704025988373),\n",
       "  np.float64(2.9663129213498673),\n",
       "  np.float64(2.964358886866741),\n",
       "  np.float64(2.962408293080334),\n",
       "  np.float64(2.9604611339322124),\n",
       "  np.float64(2.9585174033746067),\n",
       "  np.float64(2.9565770953703976),\n",
       "  np.float64(2.9546402038930943),\n",
       "  np.float64(2.9527067229268202),\n",
       "  np.float64(2.95077664646629),\n",
       "  np.float64(2.948849968516792),\n",
       "  np.float64(2.9469266830941723),\n",
       "  np.float64(2.945006784224812),\n",
       "  np.float64(2.94309026594561),\n",
       "  np.float64(2.941177122303967),\n",
       "  np.float64(2.939267347357766),\n",
       "  np.float64(2.937360935175352),\n",
       "  np.float64(2.935457879835512),\n",
       "  np.float64(2.9335581754274602),\n",
       "  np.float64(2.931661816050822),\n",
       "  np.float64(2.9297687958156082),\n",
       "  np.float64(2.9278791088422067),\n",
       "  np.float64(2.9259927492613476),\n",
       "  np.float64(2.9241097112141086),\n",
       "  np.float64(2.922229988851872),\n",
       "  np.float64(2.9203535763363284),\n",
       "  np.float64(2.9184804678394407),\n",
       "  np.float64(2.9166106575434405),\n",
       "  np.float64(2.9147441396408005),\n",
       "  np.float64(2.9128809083342158),\n",
       "  np.float64(2.9110209578365924),\n",
       "  np.float64(2.9091642823710324),\n",
       "  np.float64(2.9073108761708006),\n",
       "  np.float64(2.905460733479318),\n",
       "  np.float64(2.9036138485501475),\n",
       "  np.float64(2.9017702156469656),\n",
       "  np.float64(2.8999298290435473),\n",
       "  np.float64(2.8980926830237563),\n",
       "  np.float64(2.8962587718815205),\n",
       "  np.float64(2.8944280899208077),\n",
       "  np.float64(2.892600631455626),\n",
       "  np.float64(2.8907763908099873),\n",
       "  np.float64(2.8889553623178994),\n",
       "  np.float64(2.8871375403233537),\n",
       "  np.float64(2.885322919180292),\n",
       "  np.float64(2.8835114932525996),\n",
       "  np.float64(2.8817032569140966),\n",
       "  np.float64(2.8798982045484873),\n",
       "  np.float64(2.878096330549388),\n",
       "  np.float64(2.876297629320276),\n",
       "  np.float64(2.8745020952744835),\n",
       "  np.float64(2.8727097228351814),\n",
       "  np.float64(2.8709205064353593),\n",
       "  np.float64(2.869134440517809),\n",
       "  np.float64(2.867351519535111),\n",
       "  np.float64(2.8655717379496095),\n",
       "  np.float64(2.8637950902334035),\n",
       "  np.float64(2.8620215708683214),\n",
       "  np.float64(2.8602511743459083),\n",
       "  np.float64(2.8584838951674163),\n",
       "  np.float64(2.856719727843764),\n",
       "  np.float64(2.854958666895555),\n",
       "  np.float64(2.8532007068530287),\n",
       "  np.float64(2.851445842256053),\n",
       "  np.float64(2.8496940676541227),\n",
       "  np.float64(2.8479453776063157),\n",
       "  np.float64(2.846199766681305),\n",
       "  np.float64(2.844457229457313),\n",
       "  np.float64(2.84271776052212),\n",
       "  np.float64(2.8409813544730262),\n",
       "  np.float64(2.8392480059168554),\n",
       "  np.float64(2.8375177094699207),\n",
       "  np.float64(2.835790459758016),\n",
       "  np.float64(2.8340662514164023),\n",
       "  np.float64(2.8323450790897784),\n",
       "  np.float64(2.8306269374322857),\n",
       "  np.float64(2.8289118211074658),\n",
       "  np.float64(2.8271997247882603),\n",
       "  np.float64(2.825490643156996),\n",
       "  np.float64(2.8237845709053606),\n",
       "  np.float64(2.822081502734386),\n",
       "  np.float64(2.820381433354434),\n",
       "  np.float64(2.818684357485192),\n",
       "  np.float64(2.8169902698556246),\n",
       "  np.float64(2.8152991652039994),\n",
       "  np.float64(2.8136110382778314),\n",
       "  np.float64(2.811925883833896),\n",
       "  np.float64(2.810243696638193),\n",
       "  np.float64(2.8085644714659455),\n",
       "  np.float64(2.806888203101568),\n",
       "  np.float64(2.8052148863386703),\n",
       "  np.float64(2.8035445159800183),\n",
       "  np.float64(2.8018770868375356),\n",
       "  np.float64(2.800212593732279),\n",
       "  np.float64(2.7985510314944237),\n",
       "  np.float64(2.796892394963251),\n",
       "  np.float64(2.7952366789871266),\n",
       "  np.float64(2.7935838784234894),\n",
       "  np.float64(2.791933988138829),\n",
       "  np.float64(2.7902870030086815),\n",
       "  np.float64(2.788642917917598),\n",
       "  np.float64(2.7870017277591446),\n",
       "  np.float64(2.785363427435872),\n",
       "  np.float64(2.7837280118593113),\n",
       "  np.float64(2.7820954759499537),\n",
       "  np.float64(2.78046581463723),\n",
       "  np.float64(2.778839022859503),\n",
       "  np.float64(2.777215095564051),\n",
       "  np.float64(2.7755940277070397),\n",
       "  np.float64(2.7739758142535256),\n",
       "  np.float64(2.772360450177426),\n",
       "  np.float64(2.77074793046151),\n",
       "  np.float64(2.7691382500973787),\n",
       "  np.float64(2.7675314040854557),\n",
       "  np.float64(2.765927387434966),\n",
       "  np.float64(2.7643261951639233),\n",
       "  np.float64(2.7627278222991074),\n",
       "  np.float64(2.761132263876067),\n",
       "  np.float64(2.7595395149390827),\n",
       "  np.float64(2.757949570541163),\n",
       "  np.float64(2.756362425744034),\n",
       "  np.float64(2.754778075618107),\n",
       "  np.float64(2.753196515242478),\n",
       "  np.float64(2.7516177397049093),\n",
       "  np.float64(2.7500417441018112),\n",
       "  np.float64(2.748468523538227),\n",
       "  np.float64(2.746898073127821),\n",
       "  np.float64(2.7453303879928637),\n",
       "  np.float64(2.7437654632642063),\n",
       "  np.float64(2.7422032940812846),\n",
       "  np.float64(2.7406438755920846),\n",
       "  np.float64(2.739087202953139),\n",
       "  np.float64(2.73753327132951),\n",
       "  np.float64(2.7359820758947677),\n",
       "  np.float64(2.7344336118309895),\n",
       "  np.float64(2.732887874328729),\n",
       "  np.float64(2.7313448585870113),\n",
       "  np.float64(2.7298045598133167),\n",
       "  np.float64(2.7282669732235587),\n",
       "  np.float64(2.726732094042081),\n",
       "  np.float64(2.72519991750163),\n",
       "  np.float64(2.7236704388433566),\n",
       "  np.float64(2.7221436533167798),\n",
       "  np.float64(2.720619556179788),\n",
       "  np.float64(2.7190981426986225),\n",
       "  np.float64(2.7175794081478513),\n",
       "  np.float64(2.7160633478103744),\n",
       "  np.float64(2.714549956977393),\n",
       "  np.float64(2.7130392309483957),\n",
       "  np.float64(2.7115311650311504),\n",
       "  np.float64(2.7100257545416895),\n",
       "  np.float64(2.708522994804292),\n",
       "  np.float64(2.707022881151463),\n",
       "  np.float64(2.7055254089239402),\n",
       "  np.float64(2.704030573470656),\n",
       "  np.float64(2.702538370148729),\n",
       "  np.float64(2.701048794323462),\n",
       "  np.float64(2.699561841368315),\n",
       "  np.float64(2.698077506664894),\n",
       "  np.float64(2.696595785602938),\n",
       "  np.float64(2.695116673580301),\n",
       "  np.float64(2.6936401660029445),\n",
       "  np.float64(2.692166258284915),\n",
       "  np.float64(2.6906949458483407),\n",
       "  np.float64(2.689226224123404),\n",
       "  np.float64(2.687760088548338),\n",
       "  np.float64(2.6862965345694034),\n",
       "  np.float64(2.6848355576408847),\n",
       "  np.float64(2.6833771532250683),\n",
       "  np.float64(2.6819213167922302),\n",
       "  np.float64(2.680468043820622),\n",
       "  np.float64(2.679017329796459),\n",
       "  np.float64(2.6775691702139035),\n",
       "  np.float64(2.676123560575053),\n",
       "  np.float64(2.6746804963899207),\n",
       "  np.float64(2.673239973176432),\n",
       "  np.float64(2.6718019864604),\n",
       "  np.float64(2.6703665317755205),\n",
       "  np.float64(2.668933604663345),\n",
       "  np.float64(2.667503200673284),\n",
       "  np.float64(2.666075315362584),\n",
       "  np.float64(2.664649944296306),\n",
       "  np.float64(2.663227083047333),\n",
       "  np.float64(2.6618067271963306),\n",
       "  np.float64(2.6603888723317577),\n",
       "  np.float64(2.6589735140498294),\n",
       "  np.float64(2.6575606479545257),\n",
       "  np.float64(2.656150269657563),\n",
       "  np.float64(2.654742374778379),\n",
       "  np.float64(2.6533369589441356),\n",
       "  np.float64(2.6519340177896864),\n",
       "  np.float64(2.650533546957573),\n",
       "  np.float64(2.6491355420980107),\n",
       "  np.float64(2.647739998868875),\n",
       "  np.float64(2.646346912935684),\n",
       "  np.float64(2.6449562799715904),\n",
       "  np.float64(2.6435680956573653),\n",
       "  np.float64(2.642182355681386),\n",
       "  np.float64(2.6407990557396186),\n",
       "  np.float64(2.6394181915356096),\n",
       "  np.float64(2.6380397587804745),\n",
       "  np.float64(2.6366637531928747),\n",
       "  np.float64(2.635290170499012),\n",
       "  np.float64(2.6339190064326163),\n",
       "  np.float64(2.632550256734927),\n",
       "  np.float64(2.631183917154683),\n",
       "  np.float64(2.6298199834481104),\n",
       "  np.float64(2.6284584513789024),\n",
       "  np.float64(2.627099316718221),\n",
       "  np.float64(2.6257425752446655),\n",
       "  np.float64(2.624388222744274),\n",
       "  np.float64(2.6230362550105006),\n",
       "  np.float64(2.6216866678442083),\n",
       "  np.float64(2.6203394570536553),\n",
       "  np.float64(2.618994618454481),\n",
       "  np.float64(2.6176521478696855),\n",
       "  np.float64(2.616312041129632),\n",
       "  np.float64(2.6149742940720246),\n",
       "  np.float64(2.6136389025418927),\n",
       "  np.float64(2.6123058623915862),\n",
       "  np.float64(2.6109751694807546),\n",
       "  np.float64(2.6096468196763363),\n",
       "  np.float64(2.6083208088525573),\n",
       "  np.float64(2.6069971328908967),\n",
       "  np.float64(2.605675787680091),\n",
       "  np.float64(2.6043567691161122),\n",
       "  np.float64(2.6030400731021626),\n",
       "  np.float64(2.601725695548662),\n",
       "  np.float64(2.60041363237322),\n",
       "  np.float64(2.599103879500645),\n",
       "  np.float64(2.597796432862917),\n",
       "  np.float64(2.596491288399175),\n",
       "  np.float64(2.5951884420557145),\n",
       "  np.float64(2.5938878897859667),\n",
       "  np.float64(2.5925896275504914),\n",
       "  np.float64(2.591293651316953),\n",
       "  np.float64(2.589999957060122),\n",
       "  np.float64(2.588708540761859),\n",
       "  np.float64(2.5874193984110927),\n",
       "  np.float64(2.586132526003817),\n",
       "  np.float64(2.5848479195430816),\n",
       "  np.float64(2.583565575038965),\n",
       "  np.float64(2.5822854885085813),\n",
       "  np.float64(2.5810076559760473),\n",
       "  np.float64(2.5797320734724885),\n",
       "  np.float64(2.5784587370360157),\n",
       "  np.float64(2.57718764271172),\n",
       "  np.float64(2.575918786551646),\n",
       "  np.float64(2.5746521646147995),\n",
       "  np.float64(2.5733877729671235),\n",
       "  np.float64(2.5721256076814845),\n",
       "  np.float64(2.570865664837671),\n",
       "  np.float64(2.5696079405223684),\n",
       "  np.float64(2.568352430829153),\n",
       "  np.float64(2.5670991318584804),\n",
       "  np.float64(2.56584803971768),\n",
       "  np.float64(2.5645991505209214),\n",
       "  np.float64(2.5633524603892246),\n",
       "  np.float64(2.5621079654504437),\n",
       "  np.float64(2.560865661839242),\n",
       "  np.float64(2.5596255456970916),\n",
       "  np.float64(2.5583876131722643),\n",
       "  np.float64(2.557151860419809),\n",
       "  np.float64(2.555918283601546),\n",
       "  np.float64(2.5546868788860495),\n",
       "  np.float64(2.5534576424486524),\n",
       "  np.float64(2.5522305704714072),\n",
       "  np.float64(2.551005659143099),\n",
       "  np.float64(2.5497829046592235),\n",
       "  np.float64(2.5485623032219693),\n",
       "  np.float64(2.5473438510402167),\n",
       "  np.float64(2.5461275443295173),\n",
       "  np.float64(2.5449133793120966),\n",
       "  np.float64(2.543701352216821),\n",
       "  np.float64(2.542491459279199),\n",
       "  np.float64(2.5412836967413726),\n",
       "  np.float64(2.5400780608521023),\n",
       "  np.float64(2.538874547866741),\n",
       "  np.float64(2.537673154047251),\n",
       "  np.float64(2.5364738756621654),\n",
       "  np.float64(2.5352767089865935),\n",
       "  np.float64(2.534081650302202),\n",
       "  np.float64(2.532888695897202),\n",
       "  np.float64(2.531697842066347),\n",
       "  np.float64(2.5305090851109076),\n",
       "  np.float64(2.5293224213386725),\n",
       "  np.float64(2.528137847063927),\n",
       "  np.float64(2.526955358607453),\n",
       "  np.float64(2.525774952296501),\n",
       "  np.float64(2.5245966244647997),\n",
       "  np.float64(2.5234203714525263),\n",
       "  np.float64(2.5222461896063),\n",
       "  np.float64(2.521074075279183),\n",
       "  np.float64(2.519904024830648),\n",
       "  np.float64(2.5187360346265875),\n",
       "  np.float64(2.5175701010392837),\n",
       "  np.float64(2.516406220447414),\n",
       "  np.float64(2.5152443892360288),\n",
       "  np.float64(2.514084603796546),\n",
       "  np.float64(2.512926860526734),\n",
       "  np.float64(2.5117711558307074),\n",
       "  np.float64(2.5106174861189094),\n",
       "  np.float64(2.509465847808111),\n",
       "  np.float64(2.5083162373213796),\n",
       "  np.float64(2.507168651088093),\n",
       "  np.float64(2.5060230855439105),\n",
       "  np.float64(2.504879537130772),\n",
       "  np.float64(2.5037380022968754),\n",
       "  np.float64(2.5025984774966763),\n",
       "  np.float64(2.5014609591908767),\n",
       "  np.float64(2.500325443846405),\n",
       "  np.float64(2.4991919279364136),\n",
       "  np.float64(2.4980604079402666),\n",
       "  np.float64(2.4969308803435246),\n",
       "  np.float64(2.495803341637937),\n",
       "  np.float64(2.4946777883214284),\n",
       "  np.float64(2.4935542168980978),\n",
       "  np.float64(2.492432623878191),\n",
       "  np.float64(2.4913130057780988),\n",
       "  np.float64(2.4901953591203556),\n",
       "  np.float64(2.4890796804336093),\n",
       "  np.float64(2.487965966252626),\n",
       "  np.float64(2.4868542131182667),\n",
       "  np.float64(2.4857444175774916),\n",
       "  np.float64(2.484636576183333),\n",
       "  np.float64(2.4835306854949),\n",
       "  np.float64(2.482426742077357),\n",
       "  np.float64(2.481324742501916),\n",
       "  np.float64(2.4802246833458264),\n",
       "  np.float64(2.4791265611923667),\n",
       "  np.float64(2.4780303726308284),\n",
       "  np.float64(2.476936114256513),\n",
       "  np.float64(2.4758437826707116),\n",
       "  np.float64(2.474753374480706),\n",
       "  np.float64(2.4736648862997437),\n",
       "  np.float64(2.4725783147470457),\n",
       "  np.float64(2.4714936564477763),\n",
       "  np.float64(2.4704109080330516),\n",
       "  np.float64(2.4693300661399116),\n",
       "  np.float64(2.4682511274113197),\n",
       "  np.float64(2.4671740884961553),\n",
       "  np.float64(2.4660989460491924),\n",
       "  np.float64(2.4650256967310997),\n",
       "  np.float64(2.463954337208424),\n",
       "  np.float64(2.4628848641535814),\n",
       "  np.float64(2.4618172742448468),\n",
       "  np.float64(2.4607515641663427),\n",
       "  np.float64(2.459687730608041),\n",
       "  np.float64(2.4586257702657237),\n",
       "  np.float64(2.457565679841007),\n",
       "  np.float64(2.4565074560413063),\n",
       "  np.float64(2.455451095579837),\n",
       "  np.float64(2.454396595175599),\n",
       "  np.float64(2.453343951553379),\n",
       "  np.float64(2.452293161443717),\n",
       "  np.float64(2.4512442215829227),\n",
       "  np.float64(2.4501971287130453),\n",
       "  np.float64(2.4491518795818683),\n",
       "  np.float64(2.44810847094291),\n",
       "  np.float64(2.4470668995553986),\n",
       "  np.float64(2.446027162184272),\n",
       "  np.float64(2.4449892556001624),\n",
       "  np.float64(2.4439531765793903),\n",
       "  np.float64(2.442918921903951),\n",
       "  np.float64(2.441886488361508),\n",
       "  np.float64(2.4408558727453773),\n",
       "  np.float64(2.4398270718545256),\n",
       "  np.float64(2.438800082493553),\n",
       "  np.float64(2.4377749014726895),\n",
       "  np.float64(2.436751525607775),\n",
       "  np.float64(2.435729951720265),\n",
       "  np.float64(2.4347101766372043),\n",
       "  np.float64(2.4336921971912284),\n",
       "  np.float64(2.4326760102205522),\n",
       "  np.float64(2.4316616125689547),\n",
       "  np.float64(2.4306490010857695),\n",
       "  np.float64(2.429638172625882),\n",
       "  np.float64(2.428629124049718),\n",
       "  np.float64(2.427621852223226),\n",
       "  np.float64(2.4266163540178742),\n",
       "  np.float64(2.4256126263106434),\n",
       "  np.float64(2.4246106659840096),\n",
       "  np.float64(2.423610469925939),\n",
       "  np.float64(2.4226120350298768),\n",
       "  np.float64(2.4216153581947415),\n",
       "  np.float64(2.4206204363249113),\n",
       "  np.float64(2.41962726633021),\n",
       "  np.float64(2.418635845125908),\n",
       "  np.float64(2.417646169632706)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So let's write these beast\n",
    "\n",
    "def linear_regression_gradient_descent(X, y, alpha=0.01, n_iterations=1000):\n",
    "    # alpha is the learning rate`\n",
    "    # Convert input to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Number of training examples and features\n",
    "    m = X.shape[0]  # Number of samples\n",
    "    n = X.shape[1]  # Number of features\n",
    "\n",
    "    # Add intercept\n",
    "    ones = np.ones((m, 1))  # vector of ones\n",
    "    X = np.concatenate((ones, X), axis =1)\n",
    "\n",
    "    # Initialize theta (coefficient vector) with zeros\n",
    "    beta = np.zeros(n+1)\n",
    "\n",
    "    # Initialize cost function (optional)\n",
    "    cost_history = []\n",
    "\n",
    "    ###########################################################################\n",
    "    # Gradient Descent\n",
    "\n",
    "    for iter in range(n_iterations):\n",
    "\n",
    "        y_pred = X.dot(beta)               # Calculate predictions\n",
    "        error = y_pred - y                  # Calculate errors\n",
    "        gradients = (1/m) *  np.dot(X.T, error) # Calculate gradients\n",
    "        beta = beta - alpha * gradients   # Update rule\n",
    "\n",
    "        cost_history.append( (1/(2*m)) * np.sum(error**2)) # Optional: update cost_history \n",
    "\n",
    "    return beta, cost_history\n",
    "\n",
    "linear_regression_gradient_descent(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression.\n",
    "\n",
    "The logistic regression model arises from the desire to model the posterior \n",
    "probabilities of the K classes via linear functions in x, while at the same time\n",
    "ensuring that they sum to one and remain in [0,1].\n",
    "\n",
    "The **sigmoid function** is maps any real-valued number into a value between 0 \n",
    "and 1, making it suitable for modeling probabilities:\n",
    "\n",
    "$$   \\sigma(z) = \\frac{1}{1+ e^(-z)}  $$\n",
    "\n",
    "where $ z = \\beta_0 + \\beta X$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def logistic_regression(X, y, alpha=0.01, n_iterations=1000):\n",
    "    # alpha is the learning rate`\n",
    "    # Convert input to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Number of training examples and features\n",
    "    m = X.shape[0]  # Number of samples\n",
    "    n = X.shape[1]  # Number of features\n",
    "\n",
    "    # Add intercept\n",
    "    ones = np.ones((m, 1))  # vector of ones\n",
    "    X = np.concatenate((ones, X), axis =1)\n",
    "\n",
    "    # Initialize theta (coefficient vector) with zeros\n",
    "    beta = np.zeros(n+1)\n",
    "\n",
    "    ###########################################################################\n",
    "    # Gradient Descent\n",
    "    ###########################################################################\n",
    "\n",
    "    for iter in range(n_iterations):\n",
    "        \n",
    "        z = X.dot(beta)\n",
    "        y_pred = 1/(1 + np.exp(-z))              # Calculate predictions\n",
    "        error = y_pred - y                  # Calculate errors\n",
    "        gradients = (1/m) *  np.dot(X.T,error) # Calculate gradients\n",
    "        beta = beta - alpha * gradients   # Update rule\n",
    "\n",
    "    return beta\n",
    "\n",
    "linear_regression_gradient_descent(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def regressions(X, y, type = 'linear', alpha=0.01, n_iterations=1000):\n",
    "\n",
    "    \"\"\"\n",
    "    The implementations based on gradient descent for linear and logistic \n",
    "    regression are almost identical. The only difference is the computation of \n",
    "    y_pred. This code makes this clear, generalizing the linear regression \n",
    "    function to include a new parameter, 'type':\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : array-like\n",
    "        Feature matrix, shape (n_samples, n_features).\n",
    "    \n",
    "    y : array-like\n",
    "        Target vector, shape (n_samples,).\n",
    "    \n",
    "    type : string, optional\n",
    "        Either 'linear' or 'logistic'. Defines the type of regression performed.\n",
    "        Default is 'linear'.\n",
    "    \n",
    "    alpha : float, optional\n",
    "        Learning rate for gradient descent. Default is 0.01.\n",
    "    \n",
    "    n_iterations : int, optional\n",
    "        Number of iterations for gradient descent. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    beta : numpy.ndarray\n",
    "        Coefficient vector, shape (n_features + 1,).\n",
    "    \"\"\"\n",
    "\n",
    "    # alpha is the learning rate`\n",
    "    # Convert input to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Number of training examples and features\n",
    "    m = X.shape[0]  # Number of samples\n",
    "    n = X.shape[1]  # Number of features\n",
    "\n",
    "    # Add intercept\n",
    "    ones = np.ones((m, 1))  # vector of ones\n",
    "    X = np.concatenate((ones, X), axis =1)\n",
    "\n",
    "    # Initialize beta (coefficient vector) with zeros\n",
    "    beta = np.zeros(n+1)\n",
    "\n",
    "    ###########################################################################\n",
    "    # Gradient Descent\n",
    "    ###########################################################################\n",
    "\n",
    "    for iter in range(n_iterations):\n",
    "        \n",
    "        if type == 'linear':\n",
    "            y_pred = X.dot(beta)  \n",
    "        elif type == 'logistic':\n",
    "            z = X.dot(beta)\n",
    "            y_pred = 1/(1 + np.exp(-z))\n",
    "        else:\n",
    "            raise ValueError('Type must be either linear or logistic') \n",
    "        # Calculate predictions\n",
    "        error = y_pred - y                  # Calculate errors\n",
    "        gradients = (1/m) *  np.dot(X.T,error) # Calculate gradients\n",
    "        beta = beta - alpha * gradients   # Update rule\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0]\n",
    "n = X.shape[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional notes on logistic regression:\n",
    "\n",
    "\n",
    "Question: How would you modify your logistic regression implementation to \n",
    "include L2 regularization?\n",
    "\n",
    "We can add a regularization term to the cost function to penalize large weights.\n",
    "\n",
    "    - write deep-dive.\n",
    "\n",
    "Question: Which metrics would you use to evaluate your logistic regression model,\n",
    "and how would you implement them?\n",
    "\n",
    "Discus accuracy metrics:\n",
    "\t•\tPrecision\n",
    "\t•\tRecall\n",
    "\t•\tF1 Score\n",
    "\t•\tConfusion Matrix\n",
    "\t•\tROC Curve and AUC\n",
    "\n",
    "To-do: write code\n",
    "\n",
    "Question: Is feature scaling necessary for logistic regression? Implement \n",
    "feature scaling in your code if needed.\n",
    "\n",
    "while logistic regression doesn’t require feature scaling for correctness, \n",
    "scaling can improve the convergence rate of gradient descent. Show how to \n",
    "standardize features using z-score normalization.\n",
    "\n",
    "What are the key assumptions of logistic regression, and how can you test them \n",
    "with your data?\n",
    "\n",
    "List assumptions such as:\n",
    "\t•\tLinearity of independent variables and log-odds.\n",
    "\t•\tIndependence of errors.\n",
    "\t•\tLack of multicollinearity.\n",
    "\t•\tLarge sample size.\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
